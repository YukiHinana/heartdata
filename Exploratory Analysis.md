# Heart Disease Data

* XGBoost is an application of gradient boosted decision trees. It stands for eXtreme Gradient Boosting. It seems to be much faster than other gradient boosting techniques which is why it is so widely used. Gradient Boosting works by creating new models that predict the error of prior models. These models are then added together, it uses gradient descent to minimize the loss when adding new models.

* XGBoost is currently the favorite for most predictive analysis problems, due to its speed and effectiveness. Decision trees have long been used as a way to represent decision making in various machine learning algorithms. At some point “bagging” was developed which involves combining predictions from multiple decision trees. Boosting was the next big development, where models are built sequentially by minimizing errors from previous models. Gradient boosting is a type of boosting where the error minimization is done using gradient descent. XGBoost is a type of gradient boosting algorithm that aso implements other features such as parallel processing, tree pruning, handling missing values, and regularization to prevent overfitting or bias. Because of all these benefits, in most cases XGBoost is the best algorithm to use for predictive analysis, this is why we used it for this classification problem. 

* When we split the data into the training and testing set we used 80% of the data for training and 20% for testing. This is the industry standard split for training and testing data. We also chose to give the user a choice of if the dataset they input has its first column as indexing or row numbering. This is because if the first column of the csv is indexed, then we don't want to train our model to look for a correlation between the arbitrary indexing and the target concept. We expect the format to include all variables presented in the first column, with a target or confirmation of result in the last column, as we do not manually screen files for this format.
